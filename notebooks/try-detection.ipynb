{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# try-detection.ipynb\n",
        "\n",
        "This notebook is a **research / demo** guide showing how to run object detection on an image\n",
        "and produce cropped detection outputs suitable for storing in your app's `storage/crops/`.\n",
        "\n",
        "It is written so you can run it either with a real YOLOv8 installation (`ultralytics`) **or**\n",
        "as a simulated demo if you don't have the model weights locally.\n",
        "\n",
        "Sections:\n",
        "- Setup (packages and model weights)\n",
        "- Detection wrapper that uses YOLO when available\n",
        "- Demo run that creates example crops and prints detection metadata\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "Recommended packages (run in your environment / virtualenv):\n",
        "```\n",
        "pip install ultralytics pillow numpy opencv-python\n",
        "```\n",
        "\n",
        "- If you use a GPU, install the appropriate `torch`/CUDA packages before installing `ultralytics`.\n",
        "- Place your YOLOv8 weights (for example `yolov8s.pt`) in a safe folder and provide its path to the loader.\n",
        "\n",
        "This notebook will gracefully fall back to a simulated detection if `ultralytics` is not installed\n",
        "â€” so you can still see the data flow and cropped output generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ee556fa",
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install pillow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import json\n",
        "import uuid\n",
        "try:\n",
        "    from PIL import Image\n",
        "except Exception:\n",
        "    raise RuntimeError('Pillow is required to run this notebook; please pip install pillow')\n",
        "\n",
        "print('Notebook helpers ready')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def detect_and_crop(image_path, out_dir='generated_crops', model_path='yolov8s.pt', conf_thresh=0.25):\n",
        "    \"\"\"\n",
        "    Run object detection on `image_path` and save crops to `out_dir`.\n",
        "    If ultralytics is available and `model_path` exists, it will run real detection.\n",
        "    Otherwise it will simulate two detections so you can inspect the outputs.\n",
        "\n",
        "    Returns a list of detection metadata dicts:\n",
        "    [{ 'id':..., 'label':..., 'confidence':..., 'bbox':[x1,y1,x2,y2], 'crop_path':..., 'timestamp':... }, ...]\n",
        "    \"\"\"\n",
        "    out_dir = Path(out_dir)\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "    image_path = Path(image_path)\n",
        "    if not image_path.exists():\n",
        "        raise FileNotFoundError(f'Input image not found: {image_path}')\n",
        "\n",
        "    # Try to import ultralytics and run actual detection\n",
        "    try:\n",
        "        from ultralytics import YOLO\n",
        "        real_model = True\n",
        "    except Exception:\n",
        "        real_model = False\n",
        "\n",
        "    im = Image.open(image_path).convert('RGB')\n",
        "    w, h = im.size\n",
        "    results = []\n",
        "\n",
        "    if real_model and Path(model_path).exists():\n",
        "        print('Running real YOLO detection using model:', model_path)\n",
        "        model = YOLO(model_path)\n",
        "        preds = model(str(image_path))\n",
        "        # ultralytics returns a list of Results for each image; we have one image\n",
        "        boxes = preds[0].boxes\n",
        "        for i, box in enumerate(boxes):\n",
        "            # box.xyxy, box.conf, box.cls\n",
        "            xyxy = box.xyxy.tolist()[0]\n",
        "            conf = float(box.conf.tolist()[0])\n",
        "            cls_idx = int(box.cls.tolist()[0])\n",
        "            label = model.names.get(cls_idx, str(cls_idx)) if hasattr(model, 'names') else str(cls_idx)\n",
        "            x1, y1, x2, y2 = map(int, xyxy)\n",
        "            crop = im.crop((x1, y1, x2, y2))\n",
        "            det_id = str(uuid.uuid4())\n",
        "            crop_name = f\"{det_id}_{label}.jpg\"\n",
        "            crop_path = out_dir / crop_name\n",
        "            crop.save(crop_path)\n",
        "            meta = {\n",
        "                'id': det_id,\n",
        "                'label': label,\n",
        "                'confidence': conf,\n",
        "                'bbox': [x1, y1, x2, y2],\n",
        "                'crop_path': str(crop_path),\n",
        "                'timestamp': datetime.utcnow().isoformat() + 'Z'\n",
        "            }\n",
        "            results.append(meta)\n",
        "    else:\n",
        "        print('Ultralytics YOLO not available or model weights missing. Running simulated demo detections.')\n",
        "        # Create two simulated detections scaled to image size\n",
        "        sample_dets = [\n",
        "            { 'label': 'person', 'confidence': 0.95, 'bbox': [int(w*0.1), int(h*0.05), int(w*0.4), int(h*0.8)] },\n",
        "            { 'label': 'bench', 'confidence': 0.88, 'bbox': [int(w*0.5), int(h*0.6), int(w*0.95), int(h*0.9)] }\n",
        "        ]\n",
        "        for d in sample_dets:\n",
        "            x1, y1, x2, y2 = d['bbox']\n",
        "            crop = im.crop((x1, y1, x2, y2))\n",
        "            det_id = str(uuid.uuid4())\n",
        "            crop_name = f\"{det_id}_{d['label']}.jpg\"\n",
        "            crop_path = out_dir / crop_name\n",
        "            crop.save(crop_path)\n",
        "            meta = {\n",
        "                'id': det_id,\n",
        "                'label': d['label'],\n",
        "                'confidence': d['confidence'],\n",
        "                'bbox': [x1, y1, x2, y2],\n",
        "                'crop_path': str(crop_path),\n",
        "                'timestamp': datetime.utcnow().isoformat() + 'Z'\n",
        "            }\n",
        "            results.append(meta)\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Demo run\n",
        "\n",
        "Use the function on a test image. The repository layout expects a sample at `data/sample1.jpg`.\n",
        "If that file doesn't exist, put an image there or change the path below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "demo_img = 'data/sample1.jpg'\n",
        "if not os.path.exists(demo_img):\n",
        "    # Create a placeholder image so the demo runs even without a sample image\n",
        "    from PIL import Image, ImageDraw\n",
        "    p = Path('data')\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "    placeholder = Image.new('RGB', (640, 480), (200, 200, 200))\n",
        "    draw = ImageDraw.Draw(placeholder)\n",
        "    draw.text((20,20), 'Placeholder image for demo', fill=(0,0,0))\n",
        "    placeholder.save(demo_img)\n",
        "\n",
        "out = detect_and_crop(demo_img, out_dir='generated_crops_demo', model_path='models/weights/yolov8s.pt')\n",
        "print('Detections:')\n",
        "print(json.dumps(out, indent=2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notes & Next Steps\n",
        "\n",
        "- To run real detection, ensure `ultralytics` is installed and `model_path` points to valid YOLOv8 weights.\n",
        "- This notebook saves crops into `generated_crops_demo/` (or `generated_crops/` depending on your call).\n",
        "- After verifying detection results locally, integrate `detect_and_crop` into `app/services/detection.py`.\n",
        "- For blurriness detection: add a `blur_check.py` using variance-of-Laplacian on each crop and mark `is_blurred`.\n",
        "- Store original image + crops in the database (e.g., MongoDB GridFS) and save metadata (label, bbox, timestamp) in a collection.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e6e460e",
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "83effaf0",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import uuid\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# optional: cv2 for blur detection\n",
        "import cv2\n",
        "\n",
        "def variance_of_laplacian_gray(np_img_gray):\n",
        "    \"\"\"Return variance of Laplacian (focus measure). Higher = sharper.\"\"\"\n",
        "    return cv2.Laplacian(np_img_gray, cv2.CV_64F).var()\n",
        "\n",
        "def detect_and_crop_real(\n",
        "    image_path,\n",
        "    out_dir=\"generated_crops\",\n",
        "    model_path=\"yolov8s.pt\",    # path or known model name (yolov8s.pt, yolov8n.pt, etc.)\n",
        "    device=None,               # \"cpu\" or \"cuda:0\" or None (auto)\n",
        "    conf_thresh=0.25,\n",
        "    iou_thresh=0.45,\n",
        "    imgsz=640,\n",
        "    classes=None,              # list of class indices or names to filter (optional)\n",
        "    blur_threshold=100.0       # below this variance => considered blurred (tune as needed)\n",
        "):\n",
        "    \"\"\"\n",
        "    Run YOLOv8 detection on image_path and save crops to out_dir.\n",
        "    Returns list of metadata dicts:\n",
        "    [\n",
        "      {\n",
        "        'id': str,\n",
        "        'label': 'person',\n",
        "        'confidence': 0.93,\n",
        "        'bbox': [x1,y1,x2,y2],\n",
        "        'crop_path': '/abs/path/to/crop.jpg',\n",
        "        'timestamp': '2025-10-30T..Z',\n",
        "        'is_blurred': False\n",
        "      }, ...\n",
        "    ]\n",
        "    \"\"\"\n",
        "    out_dir = Path(out_dir)\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "    image_path = Path(image_path)\n",
        "    if not image_path.exists():\n",
        "        raise FileNotFoundError(f\"Input image not found: {image_path}\")\n",
        "\n",
        "    # Determine device\n",
        "    if device is None:\n",
        "        try:\n",
        "            import torch\n",
        "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        except Exception:\n",
        "            device = \"cpu\"\n",
        "\n",
        "    # Try to import ultralytics\n",
        "    try:\n",
        "        from ultralytics import YOLO\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(\"ultralytics is required for real detection. pip install ultralytics\") from e\n",
        "\n",
        "    # Load model (ultralytics will attempt to download weights for known names)\n",
        "    model = YOLO(str(model_path))\n",
        "\n",
        "    # Prepare classes argument: ultralytics accepts indices; mapping handled below\n",
        "    classes_arg = None\n",
        "    if classes is not None:\n",
        "        # If classes are names, convert to indices if model.names available\n",
        "        if isinstance(classes, (list, tuple)) and len(classes)>0 and isinstance(classes[0], str):\n",
        "            # build name->index map\n",
        "            name_to_idx = {v: k for k, v in model.names.items()} if hasattr(model, \"names\") else {}\n",
        "            try:\n",
        "                classes_arg = [name_to_idx[c] for c in classes]\n",
        "            except KeyError:\n",
        "                # if mapping fails, try passing None and let user specify indices\n",
        "                classes_arg = None\n",
        "        else:\n",
        "            classes_arg = classes\n",
        "\n",
        "    # Run inference\n",
        "    results = model(\n",
        "        str(image_path),\n",
        "        imgsz=imgsz,\n",
        "        device=device,\n",
        "        conf=conf_thresh,\n",
        "        iou=iou_thresh,\n",
        "        classes=classes_arg,\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    # We expect one image -> results[0]\n",
        "    res = results[0]\n",
        "    boxes = getattr(res, \"boxes\", None)\n",
        "    if boxes is None or len(boxes) == 0:\n",
        "        return []  # no detections\n",
        "\n",
        "    # read image with PIL for cropping (keeps colors correct)\n",
        "    im = Image.open(image_path).convert(\"RGB\")\n",
        "    w, h = im.size\n",
        "\n",
        "    # Extract numpy arrays from ultralytics Boxes\n",
        "    # boxes.xyxy -> Nx4 tensor; boxes.conf -> Nx1; boxes.cls -> Nx1\n",
        "    xyxy_arr = np.array(boxes.xyxy.cpu())  # shape (N,4)\n",
        "    confs = np.array(boxes.conf.cpu()).reshape(-1)\n",
        "    cls_arr = np.array(boxes.cls.cpu()).reshape(-1).astype(int)\n",
        "    name_map = model.names if hasattr(model, \"names\") else {}\n",
        "\n",
        "    results_meta = []\n",
        "    for i in range(xyxy_arr.shape[0]):\n",
        "        x1, y1, x2, y2 = xyxy_arr[i].astype(int).tolist()\n",
        "        conf = float(confs[i])\n",
        "        cls_idx = int(cls_arr[i])\n",
        "        label = name_map.get(cls_idx, str(cls_idx))\n",
        "\n",
        "        # clamp coordinates to image bounds\n",
        "        x1c = max(0, min(x1, w - 1))\n",
        "        y1c = max(0, min(y1, h - 1))\n",
        "        x2c = max(0, min(x2, w))\n",
        "        y2c = max(0, min(y2, h))\n",
        "\n",
        "        # ensure non-empty box\n",
        "        if x2c <= x1c or y2c <= y1c:\n",
        "            continue\n",
        "\n",
        "        crop = im.crop((x1c, y1c, x2c, y2c))\n",
        "        # filename: <uuid>_<label>_<timestamp>.jpg\n",
        "        det_id = str(uuid.uuid4())\n",
        "        ts = datetime.utcnow().isoformat() + \"Z\"\n",
        "        safe_label = \"\".join(c for c in label if c.isalnum() or c in (\"-\", \"_\")).lower() or \"cls\"\n",
        "        crop_name = f\"{det_id}_{safe_label}.jpg\"\n",
        "        crop_path = out_dir / crop_name\n",
        "\n",
        "        # Save crop (JPEG)\n",
        "        crop.save(crop_path, format=\"JPEG\", quality=90)\n",
        "\n",
        "        # Blur check: convert to grayscale numpy array and compute variance-of-Laplacian\n",
        "        np_crop = np.array(crop)\n",
        "        gray = cv2.cvtColor(np_crop, cv2.COLOR_RGB2GRAY)\n",
        "        focus_measure = variance_of_laplacian_gray(gray)\n",
        "        is_blurred = focus_measure < blur_threshold\n",
        "\n",
        "        meta = {\n",
        "            \"id\": det_id,\n",
        "            \"label\": label,\n",
        "            \"confidence\": conf,\n",
        "            \"bbox\": [int(x1c), int(y1c), int(x2c), int(y2c)],\n",
        "            \"crop_path\": str(crop_path),\n",
        "            \"timestamp\": ts,\n",
        "            \"is_blurred\": bool(is_blurred),\n",
        "            \"focus_measure\": float(focus_measure),\n",
        "        }\n",
        "        results_meta.append(meta)\n",
        "\n",
        "    return results_meta\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e9ebcfa9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Detections:\n",
            "[\n",
            "  {\n",
            "    \"id\": \"e9101447-24ec-4cbd-ad00-6af8b99e8cba\",\n",
            "    \"label\": \"chair\",\n",
            "    \"confidence\": 0.8306059241294861,\n",
            "    \"bbox\": [\n",
            "      1915,\n",
            "      2518,\n",
            "      3317,\n",
            "      3639\n",
            "    ],\n",
            "    \"crop_path\": \"generated_crops_real\\\\e9101447-24ec-4cbd-ad00-6af8b99e8cba_chair.jpg\",\n",
            "    \"timestamp\": \"2025-10-30T11:19:03.469115Z\",\n",
            "    \"is_blurred\": false,\n",
            "    \"focus_measure\": 1118.6570082655448\n",
            "  },\n",
            "  {\n",
            "    \"id\": \"4b4663d2-69c8-40a4-a06c-70c1443c932c\",\n",
            "    \"label\": \"bed\",\n",
            "    \"confidence\": 0.6031917333602905,\n",
            "    \"bbox\": [\n",
            "      1209,\n",
            "      2249,\n",
            "      2232,\n",
            "      2954\n",
            "    ],\n",
            "    \"crop_path\": \"generated_crops_real\\\\4b4663d2-69c8-40a4-a06c-70c1443c932c_bed.jpg\",\n",
            "    \"timestamp\": \"2025-10-30T11:19:03.537218Z\",\n",
            "    \"is_blurred\": false,\n",
            "    \"focus_measure\": 490.3846971667312\n",
            "  },\n",
            "  {\n",
            "    \"id\": \"6e28e00a-d3a3-41cb-b859-51c7f61b5aa9\",\n",
            "    \"label\": \"couch\",\n",
            "    \"confidence\": 0.5025451183319092,\n",
            "    \"bbox\": [\n",
            "      2242,\n",
            "      2192,\n",
            "      2782,\n",
            "      2504\n",
            "    ],\n",
            "    \"crop_path\": \"generated_crops_real\\\\6e28e00a-d3a3-41cb-b859-51c7f61b5aa9_couch.jpg\",\n",
            "    \"timestamp\": \"2025-10-30T11:19:03.571149Z\",\n",
            "    \"is_blurred\": false,\n",
            "    \"focus_measure\": 445.59970177710807\n",
            "  },\n",
            "  {\n",
            "    \"id\": \"ae467f1a-84f9-4a08-af9d-4fbd3d855348\",\n",
            "    \"label\": \"couch\",\n",
            "    \"confidence\": 0.43381384015083313,\n",
            "    \"bbox\": [\n",
            "      2780,\n",
            "      2208,\n",
            "      3293,\n",
            "      2614\n",
            "    ],\n",
            "    \"crop_path\": \"generated_crops_real\\\\ae467f1a-84f9-4a08-af9d-4fbd3d855348_couch.jpg\",\n",
            "    \"timestamp\": \"2025-10-30T11:19:03.580748Z\",\n",
            "    \"is_blurred\": false,\n",
            "    \"focus_measure\": 646.9375585322114\n",
            "  },\n",
            "  {\n",
            "    \"id\": \"82f83279-5cb4-4d8b-a8b8-933a5f5f2729\",\n",
            "    \"label\": \"chair\",\n",
            "    \"confidence\": 0.3197305500507355,\n",
            "    \"bbox\": [\n",
            "      1712,\n",
            "      2132,\n",
            "      1933,\n",
            "      2431\n",
            "    ],\n",
            "    \"crop_path\": \"generated_crops_real\\\\82f83279-5cb4-4d8b-a8b8-933a5f5f2729_chair.jpg\",\n",
            "    \"timestamp\": \"2025-10-30T11:19:03.590146Z\",\n",
            "    \"is_blurred\": false,\n",
            "    \"focus_measure\": 308.5779499378014\n",
            "  },\n",
            "  {\n",
            "    \"id\": \"4ddbee3c-7d0e-4ea8-83d5-dea55c683f7a\",\n",
            "    \"label\": \"couch\",\n",
            "    \"confidence\": 0.3081440329551697,\n",
            "    \"bbox\": [\n",
            "      1214,\n",
            "      2259,\n",
            "      2188,\n",
            "      2954\n",
            "    ],\n",
            "    \"crop_path\": \"generated_crops_real\\\\4ddbee3c-7d0e-4ea8-83d5-dea55c683f7a_couch.jpg\",\n",
            "    \"timestamp\": \"2025-10-30T11:19:03.595671Z\",\n",
            "    \"is_blurred\": false,\n",
            "    \"focus_measure\": 492.85671191265027\n",
            "  }\n",
            "]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\dhanu\\AppData\\Local\\Temp\\ipykernel_11556\\3694162461.py:102: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
            "  xyxy_arr = np.array(boxes.xyxy.cpu())  # shape (N,4)\n",
            "C:\\Users\\dhanu\\AppData\\Local\\Temp\\ipykernel_11556\\3694162461.py:103: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
            "  confs = np.array(boxes.conf.cpu()).reshape(-1)\n",
            "C:\\Users\\dhanu\\AppData\\Local\\Temp\\ipykernel_11556\\3694162461.py:104: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
            "  cls_arr = np.array(boxes.cls.cpu()).reshape(-1).astype(int)\n",
            "C:\\Users\\dhanu\\AppData\\Local\\Temp\\ipykernel_11556\\3694162461.py:127: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.utcnow().isoformat() + \"Z\"\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "out = detect_and_crop_real(\n",
        "    \"data/sample2.jpg\",\n",
        "    out_dir=\"generated_crops_real\",\n",
        "    model_path=\"models/weights/yolov8s.pt\",  # or \"yolov8s.pt\" (ultralytics may auto-download)\n",
        "    conf_thresh=0.3,\n",
        "    imgsz=640\n",
        ")\n",
        "\n",
        "print(\"Detections:\")\n",
        "import json\n",
        "print(json.dumps(out, indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "f2f287ba",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stored original image as: storage\\originals\\45a02db8-d3df-4eeb-98c6-14bd22cba781_sample1.jpg\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\dhanu\\AppData\\Local\\Temp\\ipykernel_11556\\1546868916.py:16: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  \"uploaded_at\": datetime.utcnow().isoformat() + \"Z\",\n"
          ]
        }
      ],
      "source": [
        "# After detection:\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "import time\n",
        "\n",
        "orig_dst = Path(\"storage/originals\")\n",
        "orig_dst.mkdir(parents=True, exist_ok=True)\n",
        "orig_id = str(uuid.uuid4())\n",
        "orig_name = orig_dst / f\"{orig_id}_{Path('data/sample1.jpg').name}\"\n",
        "shutil.copy(\"data/sample1.jpg\", orig_name)\n",
        "\n",
        "# metadata object to store in DB (example)\n",
        "doc = {\n",
        "    \"original_id\": orig_id,\n",
        "    \"original_path\": str(orig_name),\n",
        "    \"uploaded_at\": datetime.utcnow().isoformat() + \"Z\",\n",
        "    \"detections\": out  # out is the results_meta returned earlier\n",
        "}\n",
        "# Insert into Mongo/Postgres as appropriate\n",
        "print(\"Stored original image as:\", orig_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "186d1bbe",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name: ultralytics\n",
            "Version: 8.3.222\n",
            "Summary: Ultralytics YOLO ðŸš€ for SOTA object detection, multi-object tracking, instance segmentation, pose estimation and image classification.\n",
            "Home-page: https://ultralytics.com\n",
            "Author: \n",
            "Author-email: Glenn Jocher <glenn.jocher@ultralytics.com>, Jing Qiu <jing.qiu@ultralytics.com>\n",
            "License: AGPL-3.0\n",
            "Location: c:\\Users\\dhanu\\.conda\\envs\\oenv\\Lib\\site-packages\n",
            "Requires: matplotlib, numpy, opencv-python, pillow, polars, psutil, pyyaml, requests, scipy, torch, torchvision, ultralytics-thop\n",
            "Required-by: \n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip show ultralytics"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "oenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
